# jemdoc: menu{MENU}{personal.html}
= Personal Projects

Here is a summary of a couple of personal projects available on my [https://github.com/divymurli github]:
- A reproducibility study of an [https://arxiv.org/abs/1912.03263 ICLR 2020 paper] about an energy-based approach to training hybrid (simultaneously generative and discriminative) deep learning models, and submitted a [https://openreview.net/forum?id=ShrPBsjByVa&referrer=%5BML%20Reproducibility%20Challenge%202020%5D(%2Fgroup%3Fid%3DML_Reproducibility_Challenge%2F2020) report] to the 2020 ML reproducibility challenge. The reprocibility study code can be found [https://github.com/divymurli/ML_Reprod_HybridEnergyModels here].
- An implementation of the original transformer architecture from Vaswani et al (code [https://arxiv.org/abs/1706.03762 here]) for machine translation (German-English). Architecture implemented from scratch, data loaded using +torchtext+. Code available [https://github.com/divymurli/Vanilla-Transformer here].
- A fastAPI deployment of a named entity recognition model (backend BERT fined tuned on [this dataset https://www.kaggle.com/abhinavwalia95/entity-annotated-corpus]) on an AWS EC2 instance, code for finetuning and deploying can be found [here https://github.com/divymurli/NER_BERT_basic]. 
- A two-latent layer [https://arxiv.org/abs/1602.02282 ladder variational autoencoder], implemented in PyTorch. This is an extension of a homework assignment for Stefano Ermon's [https://deepgenerativemodels.github.io/ Deep Generative Models] course. The original sourcecode is written using Theano, Lasagne and Parmesan and is available [https://github.com/casperkaae/LVAE/blob/master/run_models.py here]. See also [http://orbit.dtu.dk/files/121765928/1602.02282.pdf this paper] on training ladder variational autoencoders. 

